LecturesModeling We started this course by analyzing a language model as a black box:p(x1,…,xL)orprompt⇝completionp(x1,…,xL)orprompt⇝completionThen we looked at the training data of large language models (e.g., The Pile):training data⇒p.training data⇒p.In this lecture, we will open up the onion all the way and talk about how large language models are built.Today’s lecture will focus on two topics, tokenization and model architecture.Tokenization: how a string is split into tokens.Model architecture: We will discuss mostly the Transformer architecture, which is the modeling innovation that really enabled large language models.  TokenizationRecall that a language model pp is a probability distribution over a sequence of tokens where each token comes from some vocabulary VV:[the,mouse,ate,the,cheese][the,mouse,ate,the,cheese]However, natural language doesn’t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters):the mouse ate the cheesethe mouse ate the cheeseA tokenizer converts any string into a sequence of tokens.the mouse ate the cheese⇒[the,mouse,ate,the,cheese]the mouse ate the cheese⇒[the,mouse,ate,the,cheese]This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work.  Split by spacesThe simplest solution is to do:text.split(' ')This doesn’t work for languages such as Chinese, where sentences are written without spaces between words:我今天去了商店。 [gloss: I went to the store.]Then there are languages like German that have long compound words (e.g., Abwasserbehandlungsanlange).Even in English, there are hyphenated words (e.g., father-in-law) and contractions (e.g., don’t), which should get split up. For example, the Penn Treebank splits don’t into do and n’t, a linguistically-informed but not obvious choice.Therefore, splitting by spaces by spaces to identify words is quite problematic.What makes a good tokenization?We don’t want too many tokens (extreme: characters or bytes), or else the sequence becomes difficult to model.We don’t want too few tokens, or else there won’t be parameter sharing between words (e.g., should mother-in-law and father-in-law be completely different)? This is especially problematic for morphologically rich languages (e.g., Arabic, Turkish, etc.).Each token should be a linguistically or statistically meaningful unit.  Byte pair encodingSennrich et al, 2015 applied the byte pair encoding (BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers.Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot.Input: a training corpus (sequence of characters).Initialize the vocabulary VV be the set of characters.While we want to still grow VV:Find the pair of elements x,x′∈Vx,x′∈V that co-occur the most number of times.Replace all occurrences of x,x′x,x′ with a new symbol xx′xx′.Add xx′xx′ to VV.Example:[t, h, e, ␣, c, a, r], [t, h, e, ␣, c, a, t], [t, h, e, ␣, r, a, t][th, e, ␣, c, a, r], [th, e, ␣, c, a, t], [th, e, ␣, r, a, t] (th occurs 3x)[the, ␣, c, a, r], [the, ␣, c, a, t], [the, ␣, r, a, t] (the occurs 3x)[the, ␣, ca, r], [the, ␣, ca, t], [the, ␣, r, a, t] (ca occurs 2x)The output of learning is:Updated vocabulary VV: [a, c, e, h, t, r, ca, th, the]The merges that we made (important for applying the tokenizer):t, h ⇒⇒ thth, e ⇒⇒ thec, a ⇒⇒ caApplying the tokenizer. To tokenize a new string, apply the merges in the same order:[t, h, e, ␣, o, x][th, e, ␣, o, x][the, ␣, o, x]Unicode.One problem is that (especially in the multilingual setting), there are a lot (144,697) of Unicode characters.We certainly will not see all characters in the training data.In order to reduce data sparsity even further, we can run BPE on bytes instead of Unicode characters (Wang et al. 2019).Example in Chinese:今天 [gloss: today] [x62, x11, 4e, ca]  Unigram model (SentencePiece)Rather than just splitting by frequency, a more “principled” approach is to define an objective function that captures what a good tokenization looks like. We now describe the unigram model (Kudo 2018).It was of the tokenizations supported in the SentencePiece tool (Kudo & Richardson, 2018), along with BPE.It was used to train T5 and Gopher.Given a sequence x1:Lx1:L, a tokenization TT is a set ofp(x1:L)=∏(i,j)∈Tp(xi:j).p(x1:L)=∏(i,j)∈Tp(xi:j).Example:Training data (string): ababcababcTokenization T={(1,2),(3,4),(5,5)}T={(1,2),(3,4),(5,5)} (V={ab,c}V={ab,c})Likelihood: p(x1:L)=23⋅23⋅13=49p(x1:L)=23⋅23⋅13=49.Algorithm:Start with a “reasonably big” seed vocabulary VV.Repeat:Given VV, optimize p(x)p(x) and TT using the EM algorithm.Compute loss(x)loss(x) for each token x∈Vx∈V capturing how much the likelihood would be reduced if xx were removed from VV.Sort by loss and keep the top 80% tokens in VV.  Comparing tokenizersGPT-2 and GPT-3 used BPE, vocabulary size of 50KJurassic used SentencePiece with vocabulary size of 256KImpact:Given the same string, Jurassic requires 28% fewer tokens than GPT-3, so it is 1.4x fasterBoth Jurassic and GPT-3 use the same context size (2048), so one can feed in 39% more text into the prompt.Examples of tokenizations for both GPT-3 and Jurassic (demo):GPT-3: [Ab, raham, ␣Lincoln, ␣lived, ␣at, ␣the, ␣White, ␣House, .]Jurassic: [Abraham␣Lincoln, ␣lived, ␣at␣the␣White␣House, .]  ModelsThus far, we have defined language models as a probability distribution over sequences of tokens p(x1,…,xL)p(x1,…,xL), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence.Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings:[the,mouse,ate,the,cheese]⇒ϕ[(10.1),(01),(11),(1−0.1),(0−1)].[the,mouse,ate,the,cheese]⇒ϕ[(10.1),(01),(11),(1−0.1),(0−1)].As the name suggests, the contextual embedding of a token depends on its context (surrounding words); for example, consider thethe.Notation: We will ϕ:VL→Rd×Lϕ:VL→Rd×L to be the embedding function (analogous to a feature map for sequences).For a token sequence x1:L=[x1,…,xL]x1:L=[x1,…,xL], ϕϕ produces contextual embeddings ϕ(x1:L)ϕ(x1:L).  Types of language modelsWe will broaden our notion of language models to three types of models.Encoder-only (BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text.x1:L⇒ϕ(x1:L).x1:L⇒ϕ(x1:L).These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks).Example: sentiment classification[[CLS],the,movie,was,great]⇒positive.[[CLS],the,movie,was,great]⇒positive.Example: natural language inference[[CLS],all,animals,breathe,[SEP],cats,breathe]⇒entailment.[[CLS],all,animals,breathe,[SEP],cats,breathe]⇒entailment.Pro: contextual embedding for xixi can depend bidirectionally on both the left context (x1:i−1x1:i−1) and the right context (xi+1:Lxi+1:L).Con: cannot naturally generate completions.Con: requires more ad-hoc training objectives (masked language modeling).Decoder-only (GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt x1:ix1:i produces both contextual embeddings and a distribution over next tokens xi+1xi+1 (and recursively, over the entire completion xi+1:Lxi+1:L).x1:i⇒ϕ(x1:i),p(xi+1∣x1:i).x1:i⇒ϕ(x1:i),p(xi+1∣x1:i).Example: text autocomplete[[CLS],the,movie,was]⇒great[[CLS],the,movie,was]⇒greatCon: contextual embedding for xixi can only depend unidirectionally on both the left context (x1:i−1x1:i−1).Pro: can naturally generate completions.Pro: simple training objective (maximum likelihood).Encoder-decoder (BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input x1:Lx1:L and can generate the output y1:Ly1:L.x1:L⇒ϕ(x1:L),p(y1:L∣ϕ(x1:L)).x1:L⇒ϕ(x1:L),p(y1:L∣ϕ(x1:L)).Example: table-to-text generation[name,:,Clowns,|,eatType,:,coffee,shop]⇒[Clowns,is,a,coffee,shop].[name,:,Clowns,|,eatType,:,coffee,shop]⇒[Clowns,is,a,coffee,shop].Pro: contextual embedding for xixi can depend bidirectionally on both the left context (x1:i−1x1:i−1) and the right context (xi+1:Lxi+1:L).Pro: can naturally generate outputs.Con: requires more ad-hoc training objectives.  PreliminariesWe now describe the innards of the embedding function ϕ:VL→Rd×Lϕ:VL→Rd×L:[the,mouse,ate,the,cheese]⇒ϕ[(10.1),(01),(11),(1−0.1),(0−1)].[the,mouse,ate,the,cheese]⇒ϕ[(10.1),(01),(11),(1−0.1),(0−1)].We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on these slides from CS221 on differentiable programming, and will depart a bit from the standard presentation.The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity:TransformerBlock(x1:L).TransformerBlock(x1:L).This function will have parameters which we will include in the body but elide in the function signature for simplicity.In what follows, we will define a library of building blocks until we get to the full Transformer.  PreliminariesFirst, we have to convert sequences of tokens into sequences of vectors. EmbedTokenEmbedToken does exactly this by looking up each token in an embedding matrix E∈R|V|×dE∈R|V|×d (a parameter that will be learned from data):[the,mouse,ate,the,cheese][the,mouse,ate,the,cheese]def EmbedToken(x1:L:VL)→Rd×LEmbedToken(x1:L:VL)→Rd×L:Turns each token xixi in the sequence x1:Lx1:L into a vector.Return [Ex1,…,ExL][Ex1,…,ExL].These are exactly the (context-independent) word embeddings of yore. We define an abstract SequenceModelSequenceModel function that takes these context-independent embeddings and maps them into contextual embeddings.def SequenceModel(x1:L:Rd×L)→Rd×LSequenceModel(x1:L:Rd×L)→Rd×L:Process each element xixi in the sequence x1:Lx1:L with respect to other elements.[abstract implementation (e.g., FeedForwardSequenceModelFeedForwardSequenceModel, SequenceRNNSequenceRNN, TransformerBlockTransformerBlock)]The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to a fixed length context, just as in an n-gram model:def FeedForwardSequenceModel(x1:L:Rd×L)→Rd×LFeedForwardSequenceModel(x1:L:Rd×L)→Rd×L:Process each element xixi in the sequence x1:Lx1:L by looking at the last nn elements..For each i=1,…,Li=1,…,L:Compute hi=FeedForward(xi−n+1,…,xi)hi=FeedForward(xi−n+1,…,xi).Return [h1,…,hL][h1,…,hL].  Recurrent neural networksThe first “real” sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence of hidden states recursively.def SequenceRNN(x:Rd×L)→Rd×LSequenceRNN(x:Rd×L)→Rd×L:Process the sequence x1,…,xLx1,…,xL left-to-right and recursively compute vectors h1,…,hLh1,…,hL.For i=1,…,Li=1,…,L:Compute hi=RNN(hi−1,xi)hi=RNN(hi−1,xi).Return [h1,…,hL][h1,…,hL].The actual module that does the hard work is the RNNRNN, which analogous to a finite state machine, takes the current state hh, a new observation xx, and returns the updated state:def RNN(h:Rd,x:Rd)→RdRNN(h:Rd,x:Rd)→Rd:Updates the hidden state hh based on a new observation xx.[abstract implementation (e.g., SimpleRNNSimpleRNN, LSTMLSTM, GRUGRU)]There are three ways to implement the RNNRNN. The earliest RNN is a simple RNN Elman, 1990, which takes a linear combination of hh and xx and pushes it through an elementwise non-linear function σσ (e.g., logistic σ(z)=(1+e−z)−1σ(z)=(1+e−z)−1 or more the modern ReLU σ(z)=max(0,z)σ(z)=max(0,z)).def SimpleRNN(h:Rd,x:Rd)→RdSimpleRNN(h:Rd,x:Rd)→Rd:Updates the hidden state hh based on a new observation xx by simple linear transformation and non-linearity.Return σ(Uh+Vx+b)σ(Uh+Vx+b).As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used by ELMo and ULMFiT.def BidirectionalSequenceRNN(x1:L:Rd×L)→R2d×LBidirectionalSequenceRNN(x1:L:Rd×L)→R2d×L:Process the sequence both left-to-right and right-to-left.Compute left-to-right: [h→1,…,h⃗ →L]←SequenceRNN(x1,…,xL)[h1→,…,h→L→]←SequenceRNN(x1,…,xL).Compute right-to-left: [h←L,…,h←1]←SequenceRNN(xL,…,x1)[hL←,…,h1←]←SequenceRNN(xL,…,x1).Return [h→1h←1,…,h→Lh←L][h1→h1←,…,hL→hL←].Notes:The simple RNN is difficult to train due to vanishing gradients.The Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) (both of RNNRNN) have been developed to address these.Still, even though the embedding h200h200 can depend arbitrarily far back (e.g., on x1x1), it is unlikely to depend on it in a “crisp” way (see Khandelwal et al., 2018 for more discussion).LSTMs in some sense were really what brought deep learning into full swing within NLP.We will not discuss these models in the interest of time.  TransformersNow, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models.There are great resources for learning about the Transformer:Illustrated Transformer and Illustrated GPT-2: very nice visual description of the Transformer.Annotated Transformer: Pytorch implementation of the Transformer.You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces.The crux of the Transformers are the attention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017).One can think of attention as a “soft” lookup table, where we have a query yy that we want to match against each element in a sequence x1:L=[x1,…,xL]x1:L=[x1,…,xL]:[x1,…,xL]y[x1,…,xL]yWe can think of each xixi as representing a key-value pair via linear transformations:(Wkeyxi):(Wvaluexi)(Wkeyxi):(Wvaluexi)and forming the query via another linear transformation:Wqueryy.Wqueryy.The key and the query can be compared to give a score:scorei=x⊤iW⊤keyWqueryy.scorei=xi⊤Wkey⊤Wqueryy.These scores can be exponentiated and normalized to form a probability distribution over the token positions {1,…,L}{1,…,L}:[α1,…,αL]=softmax([score1,…,scoreL]).[α1,…,αL]=softmax([score1,…,scoreL]).Then the final output is a weighted combination over the values:∑i=1Lαi(Wvaluexi).∑i=1Lαi(Wvaluexi).We can write this all succinctly in matrix form:def Attention(x1:L:Rd×L,y:Rd)→RdAttention(x1:L:Rd×L,y:Rd)→Rd:Process yy by comparing it to each xixi.Return Wvaluex1:Lsoftmax(x⊤1:LW⊤keyWqueryy/d−−√)Wvaluex1:Lsoftmax(x1:L⊤Wkey⊤Wqueryy/d).We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multiple attention heads and simply combine their outputs.def MultiHeadedAttention(x1:L:Rd×L,y:Rd)→Rd:MultiHeadedAttention(x1:L:Rd×L,y:Rd)→Rd:Process yy by comparing it to each xixi with respect to nheadsnheads aspects.Return Woutput[Attention(x1:L,y),…,Attention(x1:L,y)]nheadstimesWoutput[Attention(x1:L,y),…,Attention(x1:L,y)]⏟nheadstimes.Self-attention layer. Now we will substitute each xixi in for yy as the query argument to produce:def SelfAttention(x1:L:Rd×L)→Rd×L)SelfAttention(x1:L:Rd×L)→Rd×L):Compare each element xixi to each other element.Return [Attention(x1:L,x1),…,Attention(x1:L,xL)][Attention(x1:L,x1),…,Attention(x1:L,xL)].Feedforward layer. Self-attention allows all the tokens to “talk” to each other, whereas feedforward connections provide:def FeedForward(x1:L:Rd×L)→Rd×LFeedForward(x1:L:Rd×L)→Rd×L:Process each token independently.For i=1,…,Li=1,…,L:Compute yi=W2max(W1xi+b1,0)+b2yi=W2max(W1xi+b1,0)+b2.Return [y1,…,yL][y1,…,yL].Improving trainability. We’re almost done. We could in principle just take the FeedForward∘SelfAttentionFeedForward∘SelfAttention sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable.Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function ff:f(x1:L),f(x1:L),we add a residual (skip) connection so that if ff’s gradients vanish, gradients can still flow through x1:Lx1:L:x1:L+f(x1:L).x1:L+f(x1:L).Layer normalization. Another trick is layer normalization, which takes a takes a vector and makes sure its elements are too big:def LayerNorm(x1:L:Rd×L)→Rd×LLayerNorm(x1:L:Rd×L)→Rd×L:Make each xixi not too big or small.We first define an adapter function that takes a sequence model ff and makes it “robust”:def AddNorm(f:(Rd×L→Rd×L),x1:L:Rd×L)→Rd×LAddNorm(f:(Rd×L→Rd×L),x1:L:Rd×L)→Rd×L:Safely apply ff to x1:Lx1:L.Return LayerNorm(x1:L+f(x1:L))LayerNorm(x1:L+f(x1:L)).Finally, we can define the Transformer block succinctly as follows:def TransformerBlock(x1:L:Rd×L)→Rd×LTransformerBlock(x1:L:Rd×L)→Rd×L:Process each element xixi in context.Return AddNorm(FeedForward,AddNorm(SelfAttention,x1:L))AddNorm(FeedForward,AddNorm(SelfAttention,x1:L)).Positional embeddings. You might have noticed that as defined, the embedding of a token doesn’t depend on where it occurs in the sequence, so mousemouse in both sentences would have the same embedding, which is not sensible.[the,mouse,ate,the,cheese][the,mouse,ate,the,cheese] [the,cheese,ate,the,mouse][the,cheese,ate,the,mouse]To fix this, we add positional information into the embedding:def EmbedTokenWithPosition(x1:L:Rd×L)EmbedTokenWithPosition(x1:L:Rd×L):Add in positional information.Define positional embeddings:Even dimensions: Pi,2j=sin(i/100002j/dmodel)Pi,2j=sin⁡(i/100002j/dmodel)Odd dimensions: Pi,2j+1=cos(i/100002j/dmodel)Pi,2j+1=cos⁡(i/100002j/dmodel)Return [x1+P1,…,xL+PL][x1+P1,…,xL+PL].GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times:GPT-3(x1:L)=TransformerBlock96(EmbedTokenWithPosition(x1:L))GPT-3(x1:L)=TransformerBlock96(EmbedTokenWithPosition(x1:L))Shape of the architecture (how the 175 billion parameters are allocated):Dimension of hidden state: dmodel=12288dmodel=12288Dimension of the intermediate feed-forward layer: dff=4dmodeldff=4dmodelNumber of heads: nheads=96nheads=96Context length: L=2048L=2048These decisions are not necessarily optimal. Levine et al. 2020 provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture.There are important but detailed differences between different versions of Transformers:Layer normalization “post-norm” (original Transformers paper) versus pre-norm (GPT-2), which impacts training stability (Davis et al. 2021).Dropout is applied throughout to prevent overfitting.GPT-3 uses a sparse Transformer to reduce the number of parameters, interleaving it with dense layers.Depending on the type of Transformer (encoder-only, decoder-only, encoder-decoder), different masking operations are used.And of course there are many more details involved in the training of Transformer models which we will discuss next time.  Further readingTokenization:Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP. Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Y. Lee, Benoît Sagot, Samson Tan. 2021. Comprehensive survey of tokenization.Neural Machine Translation of Rare Words with Subword Units. Rico Sennrich, B. Haddow, Alexandra Birch. ACL 2015. Introduces byte pair encoding into NLP. Used by GPT-2, GPT-3.Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, Yuan Cao, Qin Gao, Klaus Macherey, J. Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Y. Kato, Taku Kudo, H. Kazawa, K. Stevens, George Kurian, Nishant Patil, W. Wang, C. Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, G. Corrado, Macduff Hughes, J. Dean. 2016. Introduces WordPiece. Used by BERT.SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. Taku Kudo, John Richardson. EMNLP 2018. Introduces SentencePiece.Modeling:Language Models are Unsupervised Multitask Learners. Introduces GPT-2.Attention is All you Need. Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. NIPS 2017.Illustrated TransformerCS224N slides on RNNsCS224N slides on TransformersTrain Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. Ofir Press, Noah A. Smith, M. Lewis. 2021. Introduces Alibi embeddings.Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov. ACL 2019. Introduces recurrence on Transformers, relative position encoding scheme.Generating Long Sequences with Sparse Transformers. R. Child, Scott Gray, Alec Radford, Ilya Sutskever. 2019. Introduces Sparse Transformers.Linformer: Self-Attention with Linear Complexity. Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma. 2020. Introduces Linformers.Rethinking Attention with Performers. K. Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, Adrian Weller. ICLR 2020. Introduces Performers.Efficient Transformers: A Survey. Yi Tay, M. Dehghani, Dara Bahri, Donald Metzler. 2020.Decoder-only architectures:Language Models are Unsupervised Multitask Learners. Alec Radford, Jeff Wu, R. Child, D. Luan, Dario Amodei, Ilya Sutskever. 2019. Introduces GPT-2 from OpenAI.Language Models are Few-Shot Learners. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. NeurIPS 2020. Introduces GPT-3 from OpenAI.Scaling Language Models: Methods, Analysis&Insights from Training Gopher. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, J. Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, G. V. D. Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John F. J. Mellor, I. Higgins, Antonia Creswell, Nathan McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, D. Budden, Esme Sutherland, K. Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine Li, A. Kuncoro, Aida Nematzadeh, E. Gribovskaya, Domenic Donato, Angeliki Lazaridou, A. Mensch, J. Lespiau, Maria Tsimpoukelli, N. Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, I. Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, Geoffrey Irving. 2021. Introduces Gopher from DeepMind.Jurassic-1: Technical details and evaluation. Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham. 2021. Introduces Jurassic from AI21 Labs.Encoder-only architectures:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. NAACL 2019. Introduces BERT from Google.RoBERTa: A Robustly Optimized BERT Pretraining Approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, Veselin Stoyanov. 2019. Introduces RoBERTa from Facebook.Encoder-decoder architectures:BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. M. Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer. ACL 2019. Introduces BART from Facebook.Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, Peter J. Liu. J. Mach. Learn. Res. 2019. Introduces T5 from Google.