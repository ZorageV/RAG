LecturesTraining Last lecture, we talked about the model architecture for large language models (e.g., the Transformer). In this lecture, we will discuss how to train large language models.Objective functionsOptimization algorithms  Objective functionsWe will consider objective functions for the three types of language models:Decoder-only (e.g., GPT-3): compute unidirectional contextual embeddings, generate one token at a timeEncoder-only (e.g., BERT): compute bidirectional contextual embeddingsEncoder-decoder (e.g., T5): encode input, decode outputWe can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):ϕ:VL→Rd×L.ϕ:VL→Rd×L. [the,mouse,ate,the,cheese]⇒ϕ[(10.1),(01),(11),(1−0.1),(0−1)].[the,mouse,ate,the,cheese]⇒ϕ[(10.1),(01),(11),(1−0.1),(0−1)].  Decoder-only modelsRecall that an autoregressive language model defines a conditional distribution:p(xi∣x1:i−1).p(xi∣x1:i−1).We define it as follows:Map x1:i−1x1:i−1 to contextual embeddings ϕ(x1:i−1)ϕ(x1:i−1).Apply an embedding matrix E∈RV×dE∈RV×d to obtain scores for each token Eϕ(x1:i−1)i−1Eϕ(x1:i−1)i−1.Exponentiate and normalize it to produce the distribution over xixi.Succinctly:p(xi+1∣x1:i)=softmax(Eϕ(x1:i)i).p(xi+1∣x1:i)=softmax(Eϕ(x1:i)i).Maximum likelihood. Let θθ be all the parameters of large language models.Let DD be the training data consisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function:O(θ)=∑x1:L∈D−logpθ(x1:L)=∑x1:L∈D∑i=1L−logpθ(xi∣x1:i−1).O(θ)=∑x1:L∈D−log⁡pθ(x1:L)=∑x1:L∈D∑i=1L−log⁡pθ(xi∣x1:i−1).There’s more to say about how to efficiently optimize this function, but that’s all there is for the objective.  Encoder-only modelsUnidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don’t need to generate.BERT. We will first present the BERT objective function, which contains two terms:Masked language modelingNext sentence predictionTake the example sequence for natural language inference (predict entailment, contradiction, or neutral):x1:L=[[CLS],all,animals,breathe,[SEP],cats,breathe].x1:L=[[CLS],all,animals,breathe,[SEP],cats,breathe].There are two special tokens:[CLS][CLS]: contains the embedding used to drive classification tasks[SEP][SEP]: used to tell the model where the first (e.g., premise) versus second sequence (e.g., hypothesis) are.Using our notation from the previous lecture, the BERT model is defined as:BERT(x1:L)=TransformerBlock24(EmbedTokenWithPosition(x1:L)+SentenceEmbedding(x1:L))∈Rd×L,BERT(x1:L)=TransformerBlock24(EmbedTokenWithPosition(x1:L)+SentenceEmbedding(x1:L))∈Rd×L,where SentenceEmbedding(x1:L)SentenceEmbedding(x1:L) returns one of 2 vectors depending on the sequence:eA∈RdeA∈Rd for tokens left of [SEP][SEP], andeB∈RdeB∈Rd for tokens right of [SEP][SEP].BERT-large has nheads=16nheads=16 attention heads, and a dmodel=1024dmodel=1024 dimensional model, resulting in 355M parameters.Masked language modeling. The basic idea of the masked language model is to train on the prediction problem:[the,[MASK],ate,[MASK],cheese]⇒[the,mouse,ate,the,cheese].[the,[MASK],ate,[MASK],cheese]⇒[the,mouse,ate,the,cheese].More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version x~1:Lx~1:L and try to reconstruct the original x1:Lx1:L.x~1:L⇒x1:L.x~1:L⇒x1:L.Model. We first define the model distribution that takes x~1:Lx~1:L and predicts each token independently given the contextual embedding:p(xi∣x~1:L)=softmax(Eϕ(x~1:L)i).p(xi∣x~1:L)=softmax(Eϕ(x~1:L)i).Masking function. We define a (stochastic) noising function A(x~1:L∣x1:L)A(x~1:L∣x1:L) that:x1:Loriginal⇒Ax~1:Lnoised.x1:L⏟original⇒Ax~1:L⏟noised.Here’s how AA is defined:Let I⊂{1,…,L}I⊂{1,…,L} be a random 15% of the tokens positions.For each i∈Ii∈I:With probability 0.8, set x~i←[MASK]x~i←[MASK].With probability 0.1, set x~i←xix~i←xi.With probability 0.1, set x~i←random word from Vx~i←random word from V.Reducing distribution shift. If we were to always replace chosen tokens in II with [MASK][MASK], then:During training, every input BERT would only see sequences with a [MASK][MASK].At test time, we would feed in sentences with no [MASK][MASK], resulting in a distribution shift. The heuristic fix is to replace with real words 20% of the time.Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not.[[CLS],the,mouse,ate,the,cheese,[SEP],it,was,full]⇒1[[CLS],the,mouse,ate,the,cheese,[SEP],it,was,full]⇒1.[[CLS],the,mouse,ate,the,cheese,[SEP],hello,world]⇒0[[CLS],the,mouse,ate,the,cheese,[SEP],hello,world]⇒0.We will use the embedding of the [CLS][CLS] token to make this binary classification decision.Dataset. Let DD be a set of examples (x1:L,c)(x1:L,c) constructed as follows:Let AA be a sentence from the corpus.With probability 0.5, let BB be the next sentence.With probability 0.5, let BB be a random sentence from the corpus.Let x1:L=[[CLS],A,[SEP],B]x1:L=[[CLS],A,[SEP],B].Let cc denote whether BB is the next sentence or not.Objective. Then the BERT objective is:O(θ)=∑(x1:L,c)∈DEI,x~1:L∼A(⋅∣x1:L,I)[∑i∈I−logpθ(x~i∣x1:L)]masked language modeling+−logp(c∣ϕ(x1:L)1)next sentence prediction.O(θ)=∑(x1:L,c)∈DEI,x~1:L∼A(⋅∣x1:L,I)[∑i∈I−log⁡pθ(x~i∣x1:L)]⏟masked language modeling+−log⁡p(c∣ϕ(x1:L)1)⏟next sentence prediction.We will talk about training later, but a few quick notes about BERT:BERT (along with ELMo and ULMFiT) showed that one uniform architecture (Transformer) could be used for many multiple classification tasks.BERT really transformed the NLP community into a pre-training + fine-tuning mindset.BERT showed the importance of having deeply bidirectional contextual embeddings, although it’s possible that model size and fine-tuning strategies make up for it (p-tuning).RoBERTa makes the following changes to BERT:Removed the next sentence prediction objective (found it didn’t help).Trained on more data (16GB text →→ 160GB text).Trained for longer. RoBERTa improved accuracy significantly over BERT on various benchmarks (e.g., on SQuAD 81.8 to 89.4).  Encoder-decoder modelsExample task (table-to-text generation):[name,:,Clowns,|,eatType,:,coffee,shop]⇒[Clowns,is,a,coffee,shop].[name,:,Clowns,|,eatType,:,coffee,shop]⇒[Clowns,is,a,coffee,shop].Recall that encoder-decoder models (e.g., BART, T5):Encode the input bidirectionally like BERT.Decode the output autoregressively like GPT-2.BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model.Same encoder architecture as RoBERTa (12 layers, hidden dimension 1024).Trained on same data as RoBERTa (160GB text).BART considers the following transformations A(x~1:L∣x1:L)A(x~1:L∣x1:L):  Based on BERT-scaled experiments, they decided on the following transformations for the final model:Mask 30% of tokens in a documentPermute all sentencesThey demonstrated strong results on both classification and generation tasks using fine-tuning.T5 (Text-to-Text Transfer Transformer).T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model.Tasks:Given a span of text, split at random point into input and output:[the,mouse]⇒[ate,the,cheese].[the,mouse]⇒[ate,the,cheese].This paper experimented with many different unsupervised objectives:  and found that the “i.i.d. noise, replace spans” worked well (though many objectives were similar).They also cast all classical NLP tasks in a uniform framework as “text-to-text” tasks:  Note the difference in approach to classification tasks:BERT used the embedding of the [CLS][CLS] token to predict.T5, GPT-2, GPT-3, etc. (models that can generate) cast the classification tasks in a natural language space.Notes:The paper does a thorough study of many aspects of the entire pipeline (dataset, model size, training objective, etc.).Based on the insights, they trained a 11B parameter model.  Optimization algorithmsNow we turn our attention to how to optimize the objective. For simplicity, let’s take autogressive language modeling:O(θ)=∑x1:L∈D−logpθ(x1:L).O(θ)=∑x1:L∈D−log⁡pθ(x1:L).Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches:Initialize parameters θ0θ0.Repeat:Sample a mini-batch Bt⊂DBt⊂D.Perform a gradient step:θt←θt−1−η1|Bt|∑x1:L∈Bt∇θ(−logpθ(x1:L)).θt←θt−1−η1|Bt|∑x1:L∈Bt∇θ(−log⁡pθ(x1:L)).The key concerns in optimization are:We want θθ to converge quickly to a good solution.We want the optimization to be numerically stable.We want to be memory efficient (especially for large models). These are often at odds with each other (e.g., fast convergence and cutting down on memory by low-precision produces less stable training).There are several levels that we can approach optimization:Classic optimization: second-order methods, constrained optimization, etc.Machine learning: stochastic methods, implicit regularization + early stoppingDeep learning: initialization, normalization (changes to the model architecture)Large language models: stability issues, weird learning rates While some of the intuitions (e.g., second-order methods) are still useful, there are many other unique challenges that need to be overcome for large language model training to work. Unfortunately, much of this is fairly ad-hoc and poorly understood.ADAM (adaptive moment estimation). ADAM incorporates two ideas:Use momentum (keep on moving in the same direction).Have an adaptive (different) step size for each dimension of θθ (inspiration from second-order methods).Initialize parameters θ0θ0.Initialize moments m0,v0←0m0,v0←0.Repeat:Sample a mini-batch Bt⊂DBt⊂D.Update parameters as follows.Updating parameters.Compute gradient:gt←1|Bt|∑x1:L∈Bt∇θ(−logpθ(x1:L)).gt←1|Bt|∑x1:L∈Bt∇θ(−log⁡pθ(x1:L)).Update first- and second-order moments:mt←β1mt−1+(1−β1)gtmt←β1mt−1+(1−β1)gt vt←β2vt−1+(1−β2)g2tvt←β2vt−1+(1−β2)gt2Do bias correction:m^t←mt/(1−βt1)m^t←mt/(1−β1t) v^t←vt/(1−βt2)v^t←vt/(1−β2t)Update parameters:θt←θt−1−ηm^t/(v^t−−√+ϵ).θt←θt−1−ηm^t/(v^t+ϵ).Memory. Using Adam increases the amount of storage from 2(num-params)2(num-params) (from θt,gtθt,gt) to 4(num-params)4(num-params) (from θt,gt,mt,vtθt,gt,mt,vt).AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint.Instead of storing the moments (mt,vtmt,vt) of a O(m×n)O(m×n) matrix, store row and column sums (O(m+n)O(m+n) memory) and reconstruct the matrix.Remove momentum.It was used to train T5.It can be difficult to get AdaFactor to train (see Twitter thread and blog post).Mixed-precision training is another method for reducing memory (Narang et al., 2018).Default: FP32 (32-bit floating point).Option: FP16 (16-bit floating point), but the problem is that any value less than 2−242−24 becomes 0.Solution: store master weights in FP32 and do everything else in FP16.Loss scaling: scale up loss to avoid gradients with small magnitudes.Result: Halves the memory usage.Learning rates.Normally, the learning rate decreases over time.For Transformers, we actually need to increase the learning rate (warmup).Huang et al., 2020 show that a potential reason for this is to prevent vanishing gradients from layer normalization leads to instability in Adam optimizer.Initialization.Given a matrix W∈Rm×nW∈Rm×n, the standard initialization (xavier initialization) is Wij∼N(0,1/n)Wij∼N(0,1/n), where nn is the fan-in.GPT-2 and GPT-3 scale the weights by an additional 1/N−−√1/N, where NN is the number of residual layers.T5 scales the attention matrices by an additional 1/d−−√1/d (code).For GPT-3:Adam parameters: β1=0.9β1=0.9, β2=0.95β2=0.95, ϵ=10−8ϵ=10−8.Batch size: 3.2 million tokens (~1500 sequences)Use gradient clipping (gt←gt/min(1,∥g∥2)gt←gt/min(1,‖g‖2)).Linear learning rate warmup (over first 375 million tokens).Cosine learning rate that goes down to 10% of value.Gradually increase the batch size.Weight decay 0.1.  Further readingMixed precision trainingFixing Weight Decay Regularization in Adam. I. Loshchilov, F. Hutter. 2017. Introduces AdamW.ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. ICLR 2020.DeBERTa: Decoding-enhanced BERT with Disentangled Attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. ICLR 2020.